# Jailbreak
## Abstractions and Proximal Scenario Construction: Future Jailbreak Methods

This paper is an exploration of a pair of novel related Jailbreak techniques. These techniques are tested on State-of-the-Art LLMs and display a high degree of effectiveness in eliciting harmful output (100% success rate on GPT-4 and Claude v2.0). Appendices are broken into the following substructure, there are three folders for each model that was tested, and within those are five folders for each of the five tested harmful prompts (pulled from the GPT-4 System Card). Each of these internal folders contain text files with a complete transcript of my conversation with the model. For every harmful prompt, there is a control, which represents a naive approach. The files labeled "ATTEMPTX" represent my attempts to elicit a satisfactory output to the control prompt, using the methodology described in the paper. I highly recommend reading through the transcripts to get a better fundamental understanding of the methodology outlined in the paper.
Disclaimer: I am not an expert in the field! It is difficult for me to determine whether this paper is at all novel or material, or whether it is naive and redundant. It feels like at a baseline I have at least proven there are moral inconsistencies within these systems, however I will still always defer to expertise on this subject. Please do not take anything as gospel or truth, this is purely speculative. 
